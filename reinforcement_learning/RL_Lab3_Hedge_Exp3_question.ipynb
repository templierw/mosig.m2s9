{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "name": "RL_Lab3_Hedge_Exp3_question.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1EKwCFFnyd2i",
        "5miuHwZuyd2j"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-ggBC-hyd2f"
      },
      "source": [
        "<h1 align = center> <b> Reinforcement Learning (M2 MOSIG / MSIAM)<h1>\n",
        "<h2 align = center> <strong> Lab 3: Non-stochastic Multi-armed Bandits: Hedge & Exp3 <h2>   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EKwCFFnyd2i"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this TP, we conduct several experiments on several regret-minimization algorithms in Multi-armed Bandit Problems. This notebook contains 2 main sections concerning 2 main feedback set-up: **full-information** and **bandit feedback**; in which, we study two algorithms, HEDGE and EXP3, which are variants of the Exponential Weights algorithmic template. \n",
        "\n",
        "$\n",
        "%----------------------------------------------------------------------\n",
        "%% Modifiers\n",
        "%----------------------------------------------------------------------\n",
        "%\\newcommand{\\alt}[1]{#1'}\t\t% for alternates\n",
        "\\newcommand{\\bb}[1]{\\mathbf{#1}}\t\t% for bold\n",
        "%\n",
        "%\n",
        "%----------------------------------------------------------------------\n",
        "%% Number fields\n",
        "%----------------------------------------------------------------------\n",
        "\\newcommand{\\F}{\\mathbb{F}}\t\t% generic field\n",
        "\\newcommand{\\N}{\\mathbb{N}}\t\t% for naturals\n",
        "\\newcommand{\\Z}{\\mathbb{Z}}\t\t% for integers\n",
        "\\newcommand{\\Q}{\\mathbb{Q}}\t\t% for rationals\n",
        "\\newcommand{\\R}{\\mathbb{R}}\t\t% for reals\n",
        "\\newcommand{\\CC}{\\mathbb{C}}\t\t% for complex numbers (may clash)\n",
        "%----------------------------------------------------------------------\n",
        "%% Operators\n",
        "%----------------------------------------------------------------------\n",
        "\\DeclareMathOperator{\\bigoh}{\\mathcal O}\t\t% for Landau O\n",
        "\\DeclareMathOperator{\\ess}{ess}\t\t% for essential\n",
        "\\DeclareMathOperator{\\grad}{\\nabla}\t\t% for gradient\n",
        "\\DeclareMathOperator{\\Hess}{Hess}\t\t% for Hessian\n",
        "\\DeclareMathOperator{\\Jac}{Jac}\t\t% for Hessian\n",
        "\\DeclareMathOperator{\\ind}{ind}\t\t% for index\n",
        "\\DeclareMathOperator{\\rank}{rank}\t\t% for rank\n",
        "\\DeclareMathOperator{\\sign}{sgn}\t\t% for sign\n",
        "\\DeclareMathOperator{\\Sym}{Sym}\t\t% for symmetric\n",
        "\\newcommand{\\inflow}{r}\n",
        "\\newcommand{\\socialcost}{\\mathfrak{C}}\n",
        "\\newcommand{\\opt}{\\texttt{opt}}\n",
        "\\newcommand{\\NEs}{\\texttt{NE}}\n",
        "\\newcommand{\\poa}{\\textrm{PoA}}\n",
        "\\newcommand{\\pos}{\\textrm{PoS}}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5miuHwZuyd2j"
      },
      "source": [
        "### Preliminary Libraries: This notebook requires the following libraries: NumPy, Matplotlib and Pandas. \n",
        "\n",
        "If you want to run this notebook in your local machine, you can install them with the following command:\n",
        "\n",
        "``\n",
        "pip install numpy matplotlib\n",
        "``\n",
        "\n",
        "If you use the Google Colab environment, you can you run the next cell to check if these libraries are already installed; if not, install them on the server machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HahDiRuHyd2k"
      },
      "source": [
        "colab_requirements = [\"numpy\",\"matplotlib\"]\n",
        "import sys, subprocess\n",
        "def run_subprocess_command(cmd):\n",
        "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
        "    for line in process.stdout:   print(line.decode().strip())\n",
        "        \n",
        "if \"google.colab\" in sys.modules:\n",
        "    for i in colab_requirements:\n",
        "        run_subprocess_command(\"pip install \" + i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDn_MzGmyd2l"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44kq86852awK"
      },
      "source": [
        "# Section 0. Problem Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ_BU2yY2ekl"
      },
      "source": [
        "### 0.1. The game:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPWrotRm-Zsu"
      },
      "source": [
        "In this session, we focus on the decision-making process of a **learner** (a decision-maker) who has the action set $\\mathcal{A}$ containing $m$ actions (labeled $0, \\ldots, m$):\n",
        "\n",
        "----\n",
        "**For** $n=1,2,\\ldots, T$, **do**:\n",
        "* Choose a vector $x_n \\in \\Delta(\\mathcal{A})$ (i.e., a mixed strategy) indicating the probabily of choosing each action.  \n",
        "* Draw and play an action $a_n \\sim x_n$\n",
        "* Encouter a payoff vectors $v_n \\in \\mathbb{R}^{m}$ and receives a payoff $v_{n,a_n}$.\n",
        "* Observe a feedback on $v_n$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucmUtzJV2ekm"
      },
      "source": [
        "### 0.2. Payoff vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaEH7cug2ekm"
      },
      "source": [
        "**Remark**: Naturally, we would like to simulate a problem set-up where payoff vectors depend on decisions of the learner and of several adversarial opponents. However, it is not useful to define a specific game; instead, **at each time $n$, we generate $v_n$ randomly** (a priori hidden from the learner). \n",
        "\n",
        "\n",
        "\n",
        "In the next cell, we write the code for (randomly) generating a matrix with dimension $T \\times n$ such that the $t$-th row of the matrix is the pure payoff vector $v_n$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu-LGst4At3F"
      },
      "source": [
        "def payoff_vector_gen(T,m):\n",
        "    payoff = np.empty((T,m))    # initialize payoff matrix \n",
        "    for n in range(T): # For each time epoch n\n",
        "        mu = np.random.uniform(low=0, high=10, size=m)    # Choose randomly m mean \n",
        "        StdVar = np.random.uniform(low=0.0, high=2, size=m)    # Choose randomly m variance\n",
        "        for a in range(m):    # For each action\n",
        "            payoff[n,a] =  np.random.normal(mu[a],StdVar[a],1)  # Generate 1 random payoff from N(mu[a],StdVar[a]) & assign it to payoff matrix\n",
        "    return payoff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaW_nAoIP8FG"
      },
      "source": [
        "We generate the payoff vectors for a game with $m=10$ actions and the time horizon $T= 10000$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPA3oxyTRNAw"
      },
      "source": [
        "np.random.seed(15)    # Fix a random seed so that we all have the same payoff vectors\n",
        "m=10\n",
        "T=10000\n",
        "payoff_vec = payoff_vector_gen(T,m)\n",
        "print(payoff_vec)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKRW9VLw2eko"
      },
      "source": [
        "### 0.3. Tools to compute regret:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjST9wc6YelW"
      },
      "source": [
        "In the sequel, we will measure the performance of algorithms via the (realized) regret:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\textrm{Reg} (T) = \\max_{a \\in \\mathcal{A}} \\sum_{n=1}^T \\textrm{payoff}[n,a] - \\textrm{payoff}[n,a_n].\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BCeMtnj2eko"
      },
      "source": [
        "\n",
        "To compute the regret, we need to look for a **best-action in hindsight** which is simply the action maximizing the total payoff if its is used repeatedly. We do this in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wNCdYD2Y6eH"
      },
      "source": [
        "best_action = 0\n",
        "best_cumul_payoff = -np.Inf\n",
        "for a in range(m):  # For each action\n",
        "    cumul_payoff = np.sum(payoff_vec[:,a])\n",
        "    if cumul_payoff > best_cumul_payoff:\n",
        "        best_action = int(a)\n",
        "        best_cumul_payoff = cumul_payoff\n",
        "\n",
        "print('Best action is: ', best_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLg1aljiappl"
      },
      "source": [
        "Finally, we define a function to compute the realized regret of a given history of play (i.e., a policy):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfrA72lRWoDd"
      },
      "source": [
        "def regret(history_play):\n",
        "    regret = 0\n",
        "    regret_history = []\n",
        "    for n in range(T): \n",
        "        regret += float(payoff_vec[n,best_action] - payoff_vec[n,history_play[n]])\n",
        "        regret_history.append(regret)\n",
        "    return np.array(regret_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bY1soFqyd2l"
      },
      "source": [
        "# Section 1: HEDGE algorithm: Online Learning with full, exact information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9p0aWnrKLAe"
      },
      "source": [
        "In this section, we code the HEDGE algorithm as follows:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dongquan-vu/teaching/main/HEDGE.PNG\" style=\"float: left\"  width= \"800x\" />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzmYVqSeLyY2"
      },
      "source": [
        "In order to code the ExpWeight algorithm, in the next cell, we prepare the logit-mapping $\\Lambda: \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$ defined as:\n",
        "\\begin{equation}\n",
        "  \\Lambda_a(y)= \\underbrace{\\frac{\\exp(y_a)}{\\sum_{a^{\\prime} \\in \\mathcal{A}} \\exp(y_{a^{\\prime}})}}_{\\textrm{computer does not like this form}} = \\underbrace{\\frac{1}{{\\sum_{a^{\\prime} \\in \\mathcal{A}} \\exp(y_{a^{\\prime}} - y_a  )}}}_{\\textrm{computer can handle this}}.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18_kyGZm2enh"
      },
      "source": [
        "def logit_mapping(y):  #taking an np.array y as input\n",
        "    output = np.zeros(m)  #initialize to store output\n",
        "    for a in range(m):\n",
        "        output[a] = 1/ np.sum(np.exp(y - y[a]))\n",
        "    return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqgFYIRB2ekq"
      },
      "source": [
        "**RECALL**: The expected regret of HEDGE is upper-bounded as follows:\n",
        "\n",
        " \n",
        "$$\n",
        " \\texttt{Reg}_T(\\texttt{Hedge}) \\le \\sqrt{2 \\log(|\\mathcal{A}|) \\cdot T} = \\mathcal{O}(\\sqrt{T})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjv0GG5z2ekq"
      },
      "source": [
        "### QUESTION 1.1: In the next cell, we code the HEDGE algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdrCKnB4b6JZ"
      },
      "source": [
        "def Hedge(m,T,gamma_init=0.1):\n",
        "    y = np.zeros(m) # Initialize y\n",
        "    gamma = gamma_init  # Initialize step-size\n",
        "    history_play = [] # To store the history of plays\n",
        "    for n in range(T):  #For each time epoch n\n",
        "        ##### BEGIN EXERCISE ###########\n",
        "        # Step 1: Compute a mixed strategy by logit-mapping:\n",
        "        x_n =  #.........\n",
        "        # Step 2: Draw a pure strategy from x_n:\n",
        "        a_n =  #.........# Choose a number in list(range(m)) with prob distribution x_n\n",
        "        # Step 4: Feedback\n",
        "        V_n =  payoff_vec[n]\n",
        "        # Step 5: Update y\n",
        "        y = #...................\n",
        "        ######## END OF EXERCISE #######\n",
        "        #Record the played action:\n",
        "        history_play.append(int(a_n))\n",
        "        gamma = gamma_init/(1+ np.sqrt(n))\n",
        "    return history_play"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SZsYz9kXlhc"
      },
      "source": [
        "### Visualize regret upper-bound of HEDGE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV6e3beqXpRF"
      },
      "source": [
        "In the next cell, we run ``Hedge`` in several episodes and record the (realized) regret. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mNCKrLncESW"
      },
      "source": [
        "N_episodes = 20\n",
        "sum_regret_hedge, avg_regret_hedge = np.array([0]*T),np.array([0]*T)\n",
        "for episode_ind in range(N_episodes):\n",
        "    print(\"\\rCount episode \", episode_ind, end=\"\")\n",
        "    hedge_result  = Hedge(m,T,gamma_init=10e-5)\n",
        "    regret_episode = regret(hedge_result)\n",
        "    if episode_ind==0:\n",
        "        min_regret_hedge = np.copy(regret_episode)\n",
        "        max_regret_hedge = np.copy(regret_episode)\n",
        "    sum_regret_hedge = sum_regret_hedge +  np.copy(regret_episode)\n",
        "    min_regret_hedge = np.minimum(min_regret_hedge,regret_episode)\n",
        "    max_regret_hedge = np.maximum(max_regret_hedge,regret_episode)\n",
        "avg_regret_hedge = np.copy(sum_regret_hedge)/N_episodes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6of4MDV2ekr"
      },
      "source": [
        "We then plot out the mean regret (taken averagely from these episodes) representing by the red line. The pink region shows the variance of the realized regret.\n",
        "\n",
        "\n",
        "**Remark**: We use log-log plot to improve the visualibility. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUjdO4KWcZF_"
      },
      "source": [
        "# Plot out:\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(avg_regret_hedge, color ='red')\n",
        "ax.fill_between(list(range(T)), min_regret_hedge,max_regret_hedge,  color ='red', alpha=0.3,linewidth=1)\n",
        "ax.grid()\n",
        "\n",
        "# ax.plot(sqrt_t)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.plot()\n",
        "ax.set_ylim(10e-3,10e2)\n",
        "ax.set_title('Realized Regret of HEDGE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw6yYz7C2ekr"
      },
      "source": [
        "### Question 1.2:  Analyze the plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2QKPcDj2ekr"
      },
      "source": [
        "* 1.2.1) From the above plot, can we know for sure the order of an upper-bound? \n",
        "\n",
        "\n",
        "\n",
        "* 1.2.2) Assume that ```average_regret_hedge```$ \\le \\mathcal{O} (n^{p})$, then what do you think would happen if we plot out: ```average_regret_hedge``` $\\big/ n^{p}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw8QcfYQ2ekr"
      },
      "source": [
        "Write your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeJmF0aY2ekr"
      },
      "source": [
        "### Question 1.3:  Having a better plot:\n",
        "In the next cell, you need to plot out the evolution of ```average_regret_hedge```   $\\big/ \\sqrt{n}$, what can you conclude from this plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIZcjzxZ2ekr"
      },
      "source": [
        "# Plot out:\n",
        "fig,ax = plt.subplots()\n",
        "t_sqrt = (1+ np.power(np.array(range(T)),0.5))\n",
        "\n",
        "\n",
        "ax.plot((avg_regret_hedge) / t_sqrt, color ='red')\n",
        "ax.grid()\n",
        "\n",
        "# ax.plot(sqrt_t)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim(10e-3,10e2)\n",
        "ax.set_title('(Expected Regret of HEDGE)' + '$ \\cdot  \\sqrt{n}$' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ArJcRL2eks"
      },
      "source": [
        "### Bonus question: To make sure about the order of the regret's upper-bound, \n",
        "Try to plot out ```average_regret_fi```   $\\big/ n^{0.7}$ and ```average_regret_fi```   $\\big/ n^{0.3}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6eB0uqk2eks"
      },
      "source": [
        "## Answer for bonus question goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b74ssaeQYit0"
      },
      "source": [
        "# SECTION 2: EXP3 Algorithm - Online Learning with Bandit Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TOJT5jMZ07e"
      },
      "source": [
        "In this section, we study the bandit feedback setting: instead of observing the whole vector $V_n$ as in previous section, the learner only observes the payoff corresponding to the played pure action $a_n$. \n",
        "\n",
        "In this setting, we need to run \"re-construct\" the payoff-vector from this partial information, via the following **Important weights estimators (IWE)**:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\hat{v}_{a,n} = \\left\\{\\begin{array}{lr}\n",
        "        &{v_{a,n}} \\big/ {x_a} &, \\textrm{ if } a = a_n \\textrm{(i.e., if a is drawn)}\\\\\n",
        "        &0  &,\\textrm{ otherwise } \n",
        "        \\end{array}\\right.\n",
        "\\end{equation}\n",
        "<img src=\"https://raw.githubusercontent.com/dongquan-vu/teaching/main/EXP3.PNG\" style=\"float: left\"  width= \"600x\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1kCzT6A2eks"
      },
      "source": [
        "**RECALL**: The expected regret of Exp3 is upper-bounded as follows:\n",
        "\n",
        "$$\n",
        " \\texttt{Reg}_T(\\texttt{Exp3}) \\le 2\\sqrt{A\\log(|\\mathcal{A}|) \\cdot T} = \\mathcal{O}(\\sqrt{|\\mathcal{A}|} \\cdot \\sqrt{T})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DpPBDwSmW31"
      },
      "source": [
        "## Question 2.1. In the next cell, we code Exp3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWIQDwzSmbqt"
      },
      "source": [
        "In the next cell, write the code of ExpWeights that uses IWE instead of the full exact payoff vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ghzy21_hCVz"
      },
      "source": [
        "def Exp3(m,T,gamma_init=0.01):\n",
        "    y = np.zeros(m) # Initialize y\n",
        "    gamma = gamma_init  # Initialize step-size\n",
        "    history_play = [] # To store the history of plays\n",
        "    for n in range(T):  #For each time epoch n\n",
        "        ##### BEGIN EXERCISE ###########\n",
        "        # Step 1: Compute a mixed strategy by logit-mapping:\n",
        "        x_n = #.............. \n",
        "        # Step 2: Draw a pure strategy from x_n:\n",
        "        a_n = #............... # Choose a number in list(range(m)) with prob distribution x_n\n",
        "        # Step 4: Feedback via IWE:\n",
        "        V_hat_n #.....................\n",
        "        # Step 5: Update y\n",
        "        y = #.....................\n",
        "        ######## END OF EXERCISE #######\n",
        "        #Record the played action:\n",
        "        history_play.append(int(a_n))\n",
        "        gamma = gamma_init/(1+ np.sqrt(n))\n",
        "    return history_play"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUdpvbQG2ekt"
      },
      "source": [
        "In the next cell, we run Exp3 in several epsiodes, record the average regret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhVq1g7V2ekt"
      },
      "source": [
        "N_episodes = 20\n",
        "sum_regret_exp3, avg_regret_exp3 = np.array([0]*T),np.array([0]*T)\n",
        "for episode_ind in range(N_episodes):\n",
        "    print(\"\\rCount episode \", episode_ind, end=\"\")\n",
        "    exp3_result  = Exp3(m,T,gamma_init=10e-5)\n",
        "    regret_episode = regret(exp3_result)\n",
        "    if episode_ind==0:\n",
        "        min_regret_exp3 = np.copy(regret_episode)\n",
        "        max_regret_exp3 = np.copy(regret_episode)\n",
        "    sum_regret_exp3 = sum_regret_exp3 +  np.copy(regret_episode)\n",
        "    min_regret_exp3 = np.minimum(min_regret_exp3,regret_episode)\n",
        "    max_regret_exp3 = np.maximum(max_regret_exp3,regret_episode)\n",
        "avg_regret_exp3 = np.copy(sum_regret_exp3)/N_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6jQpwji2ekt"
      },
      "source": [
        "### Question 2.2. Visualize regret upper-bound of Exp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80PTKe9Y2ekt"
      },
      "source": [
        "In the next cell, we want to see the upper-bound of the expected regret of Exp3.\n",
        "\n",
        "From what you have seen in the case of Hedge, what should you plot out?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3FM4uio01HU"
      },
      "source": [
        "# Plot out:\n",
        "fig,ax = plt.subplots()\n",
        "\n",
        "#Begin exercise:\n",
        "\n",
        "# FILL IN THE BLANK IN THE COMMAND BELOW:\n",
        "ax.plot(     , color ='green')\n",
        "\n",
        "#End exercise\n",
        "\n",
        "\n",
        "ax.grid()\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.plot()\n",
        "ax.set_ylim(10e-3,10e1)\n",
        "ax.set_title('(Expected Regret of Exp3)' + '$ \\cdot  \\sqrt{n}$' )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}