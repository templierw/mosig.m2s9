{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Create Assignment","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"RL_Lab2_TDLearning.ipynb","provenance":[{"file_id":"1H_ElIZLXD84Ft5RlLPLRYf6NJqW1LeIk","timestamp":1635292510052},{"file_id":"1kORrOnO7C1Lzo_6ipFsXlz5BXNBWHf2U","timestamp":1635292470617}],"collapsed_sections":["r3swasJ52z5d","L6goA-Of2z5q","fC_udmCyHZtR","7GT37eXf2z53","IrCnxHLgHNgn","z8Eh5Ppq2z59","5m_10Pgz2z6H"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"8j9rPOpq2z5Z"},"source":["<h1 align = center> <b> Reinforcement Learning (M2 MOSIG / MSIAM)<h1>\n","\n","<h1 align = center>  Lab 2: Temporal Difference Learning: Q-Learning and SARSA <h1>\n"]},{"cell_type":"markdown","metadata":{"id":"r3swasJ52z5d"},"source":["## Introduction\n","\n","The purpose of this lab is to practice with *Temporal Difference Learning* method (i.e., *TD Learning*); including two methods: *Q-Learning* and *SARSA*.\n","\n","We prepare the libraries needed for this lab by running the next 3 cells:\n","\n","$\n","\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n","\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n","\\newcommand{\\prob}{\\mathbb{P}}\n","\\def\\U{V}\n","\\def\\action{\\vs{a}}       % action\n","\\def\\A{\\mathcal{A}}        % TODO\n","\\def\\actionset{\\mathcal{A}} %%%\n","\\def\\discount{\\gamma}  % discount factor\n","\\def\\state{\\vs{s}}         % state\n","\\def\\S{\\mathcal{S}}         % TODO\n","\\def\\stateset{\\mathcal{S}}  %%%\n","%\n","\\def\\E{\\mathbb{E}}\n","%\\newcommand{transition}{T(s,a,s')}\n","%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n","\\newcommand{transitionfunc}{P}\n","\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n","\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n","\\newcommand{rewardfunc}{r}\n","\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n","\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n","\\newcommand{statespace}{\\mathcal{S}}\n","\\newcommand{statespaceterm}{\\mathcal{S}^F}\n","\\newcommand{statespacefull}{\\mathcal{S^+}}\n","\\newcommand{actionspace}{\\mathcal{A}}\n","\\newcommand{reward}{R}\n","\\newcommand{statet}{S}\n","\\newcommand{actiont}{A}\n","\\newcommand{newstatet}{S'}\n","\\newcommand{nextstate}{\\state'}\n","\\newcommand{newactiont}{A'}\n","\\newcommand{stepsize}{\\alpha}\n","\\newcommand{discount}{\\gamma}\n","\\newcommand{qtable}{Q}\n","\\newcommand{finalstate}{\\state_F}\n","%\n","\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n","\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n","\\def\\vit{Value Iteration}\n","\\def\\pit{Policy Iteration}\n","\\def\\discount{\\gamma}  % discount factor\n","\\def\\state{\\vs{s}}         % state\n","\\def\\S{\\mathcal{S}}         % TODO\n","\\def\\stateset{\\mathcal{S}}  %%%\n","\\def\\cstateset{\\mathcal{X}} %%%\n","\\def\\x{\\vs{x}}                    % TODO cstate\n","\\def\\cstate{\\vs{x}}               %%%\n","\\def\\policy{\\pi}\n","\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n","\\def\\action{\\vs{a}}       % action\n","\\def\\A{\\mathcal{A}}        % TODO\n","\\def\\actionset{\\mathcal{A}} %%%\n","\\def\\caction{\\vs{u}}       % action\n","\\def\\cactionset{\\mathcal{U}} %%%\n","\\def\\decision{\\vs{d}}       % decision\n","\\def\\randvar{\\vs{\\omega}}       %%%\n","\\def\\randset{\\Omega}       %%%\n","\\def\\transition{T}       %%%\n","\\def\\immediatereward{r}    %%%\n","\\def\\strategichorizon{s}    %%% % TODO\n","\\def\\tacticalhorizon{k}    %%%  % TODO\n","\\def\\operationalhorizon{h}    %%%\n","\\def\\constalpha{a}    %%%\n","\\def\\U{V}              % utility function\n","\\def\\valuefunc{V}\n","\\def\\X{\\mathcal{X}}\n","\\def\\meu{Maximum Expected Utility}\n","\\def\\finaltime{T}\n","\\def\\timeindex{t}\n","\\def\\iterationindex{i}\n","\\def\\decisionfunc{d}       % action\n","\\def\\mdp{\\text{MDP}}\n","$"]},{"cell_type":"markdown","metadata":{"id":"R92KteO02z5i"},"source":["**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas and Seaborn. There are two options:\n","\n","  * If you want to run this notebook in your local machine, you can install them with the following command (and ignore the next cell):\n","\n","``\n","pip install gym numpy pandas seaborn\n","``\n","\n","  * Alternatively, if you want to run this notebook on Google Colab, just run the next cell. "]},{"cell_type":"code","metadata":{"id":"3l00D9AR2z5j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635287757572,"user_tz":-120,"elapsed":11928,"user":{"displayName":"DQ Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_IXEBvreKYiPMLWeNs3AtQG-oAXCAsNwzToKX=s64","userId":"12723420442069695120"}},"outputId":"11c9f0e6-e33d-4b32-ce8e-e92c065c3e86"},"source":["colab_requirements = [\"gym\", \"numpy\",\"pandas\",\"seaborn\"]\n","import sys, subprocess\n","def run_subprocess_command(cmd):\n","    # run the command\n","    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n","    # print the output\n","    for line in process.stdout:\n","        print(line.decode().strip())\n","        \n","if \"google.colab\" in sys.modules:\n","    for i in colab_requirements:\n","        run_subprocess_command(\"pip install \" + i)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n"]}]},{"cell_type":"code","metadata":{"id":"e09EsSKh2z5o"},"source":["%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import math\n","import gym\n","import numpy as np\n","import copy\n","import pandas as pd\n","import seaborn as sns\n","sns.set_context(\"talk\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6goA-Of2z5q"},"source":["## Setup the FrozenLake toy problem with OpenAI Gym"]},{"cell_type":"markdown","metadata":{"id":"hd3qXATb2z5r"},"source":["Similar to Lab1, we will use the FrozenLake toy problem as a testbed for our algorithms. A quick remind of FrozenLake is as follows (for more details, please refer to Lab 1's notebooks):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMhrlBsL8AKx","executionInfo":{"status":"ok","timestamp":1635287758950,"user_tz":-120,"elapsed":7,"user":{"displayName":"DQ Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_IXEBvreKYiPMLWeNs3AtQG-oAXCAsNwzToKX=s64","userId":"12723420442069695120"}},"outputId":"1b45e13d-1155-470e-e1ed-fff95fcb978b"},"source":["## Load the FrozenLake environment, its states and actions:\n","env = gym.make('FrozenLake-v0')\n","# env = gym.make('FrozenLake-v1') ## Uncommentize this if you install the latest version of GymAI on your computer\n","states = list(range(env.observation_space.n))\n","actions = list(range(env.action_space.n))\n","is_final_array = np.full(shape=len(states), fill_value=np.nan, dtype=np.bool)\n","reward_array = np.full(shape=len(states), fill_value=np.NINF)                # np.NINF = negative infinity\n","transition_array = np.zeros(shape=(len(states), len(actions), len(states)))\n","for state in states:\n","    for action in actions:\n","        for next_state_tuple in env.P[state][action]:              # env.P[state][action] contains the next states list (a list of tuples)\n","            transition_probability, next_state, next_state_reward, next_state_is_final = next_state_tuple\n","            is_final_array[next_state] = next_state_is_final\n","            reward_array[next_state] = max(reward_array[next_state], next_state_reward)   # workaround: when we already are in state 15, reward is 0 if we stay in state 15 (in practice this never append as the simulation stop when we arrive in state 15 as any other terminal state)\n","            transition_array[state, action, next_state] += transition_probability\n","action_labels = { 0: \"Move Left\",   1: \"Move Down\",    2: \"Move Right\",    3: \"Move Up\"}\n","# For visualizing the environment\n","try: env.render()\n","except:pass # render not available\n","print('\\n', 'states: ', states)\n","print('action name convetion', action_labels)\n","print('Reward:' ,reward_array)\n","print('Is final state:', is_final_array)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n"," states:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n","action name convetion {0: 'Move Left', 1: 'Move Down', 2: 'Move Right', 3: 'Move Up'}\n","Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","Is final state: [False False False False False  True False  True False False False  True\n","  True False False  True]\n"]}]},{"cell_type":"markdown","metadata":{"id":"nyHhLMrv_or7"},"source":["\n","<img src=\"https://raw.githubusercontent.com/dongquan-vu/teaching/main/FrozenLake_trans.PNG\" style=\"float: left; width: 90%\" />"]},{"cell_type":"markdown","metadata":{"id":"ydYZ7QrK2z5s"},"source":["#### **Important Remark:**\n","* In Lab1, we work with **model-based** methods: the learning agent \"knows\" the transition probabilities and the rewards; more precisely, it can \"*ask for expectation*\".\n","\n","* **In this lab, we work with model-free** methods: this time, we assume that the learning agent *does not know* the world set-up; it can only \"*ask for a sample of the next state and reward*\". "]},{"cell_type":"markdown","metadata":{"id":"PUeoiXyz2z5s"},"source":["#### Extra functions for displaying results:\n","\n","In the next cell, we write two function for displaying the states and Qtables in the FrozenLake environment."]},{"cell_type":"code","metadata":{"id":"Pi1DKdHK2z5t"},"source":["def qtable_display(q_array, title=None, figsize=(4,4), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\"):\n","    num_actions = q_array.shape[1]\n","    global_figsize = list(figsize)\n","    global_figsize[0] *= num_actions\n","    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)   # Sample figsize in inches\n","    for action_index in range(num_actions):\n","        ax = ax_list[action_index]\n","        state_seq = q_array.T[action_index]\n","        states_display(state_seq, title=None, figsize=figsize, annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\", ax=ax)\n","    plt.suptitle(title)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJIU4rM82z5u"},"source":["def states_display(state_seq, title=None, figsize=(5,5), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\", ax=None):\n","    size = int(math.sqrt(len(state_seq)))\n","    state_array = np.array(state_seq)\n","    state_array = state_array.reshape(size, size)\n","    if ax is None:\n","        fig, ax = plt.subplots(figsize=figsize)         # Sample figsize in inches\n","    sns.heatmap(state_array, annot=annot, fmt=fmt, linewidths=linewidths, square=square, cbar=cbar, cmap=cmap, ax=ax)\n","    if title is not None:     plt.title(title)\n","    if ax is None: plt.show()\n","    else: return ax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7WQZFYKh2z5v"},"source":["# Section 1: Temporal Difference for Value Estimation"]},{"cell_type":"markdown","metadata":{"id":"fC_udmCyHZtR"},"source":["## TD-LEARNING algorithm:"]},{"cell_type":"markdown","metadata":{"id":"xMfTdJI9-1JO"},"source":["Recall: In order to evaluate a policy $\\pi$, in previous lab:\n","* Bellman Equation:\n"," $$\n","  \\U_{\\pi}(s) = \\mathbb{E} \\left[ \\reward(S_{t+1}) + \\discount \\U_{\\pi}(S_{t+1}) | S_t = \\state, A_t \\sim \\pi(\\state) \\right]\n","  $$\n","\n","* In Lab1: approximating $\\U_{\\pi}$ by interating (turning Bellman's equation into an update rule) \n","    \\begin{equation}\n","      \\U_{t+1}(\\state) =   \\mathbb{E} \\left[ \\reward(S_{t+1}) + \\discount \\U_{k}(S_{t+1}) | S_t = \\state, A_t \\sim \\pi(\\state) \\right] .\n","    \\end{equation}\n","\n"," <h4 align = center> <b> Key question: How to approximate Value Function using model-free methods?<h4>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O-OMU59D2z5v"},"source":["* Cannot find $\\mathbb{E}$, can we use a **sample** instead: $\\U_{t}(s) = \\overbrace{R(S_{t+1}) + \\gamma \\U_{k}(S_{t+1})}^{\\textrm{get from sampling}}$?\n","\n"," $\\Rightarrow$ This is likely very noisy. \n","\n","* **The main idea of TD-Learning:** Take a small step---controled via parameter $\\alpha$---by using the temporal difference:\n","\n","\\begin{equation}\n","      \\U_{t+1}(S_{k+1}) =  \\U_{k}(S_t) + \\alpha \\cdot \\left[\\underbrace{\\overbrace{\\reward(S_{t+1}) + \\discount \\U_{k}(S_{t+1})}^{\\textrm{target}} - \\U_{k}(S_t)}_{\\textrm{called the TD error}} \\right] .\n","\\end{equation}\n","Use this to update the Value Approximation."]},{"cell_type":"markdown","metadata":{"id":"Yh-rrW_PGtDo"},"source":["PSEUDO-CODE OF TD LEARNING:\n","\n","\n","**Input**:$\\quad\\quad$ policy $\\policy$ (to be evaluated), step size $\\stepsize \\in (0,1]$\n","\n","**Initialize** $\\valuefunc(\\state)=0, \\forall \\state \\in \\statespace$<br>\n","\n","<b>FOR EACH</b> episode<br>\n","\t$\\quad$ $\\statet \\leftarrow \\text{env.reset}()$<br>\n","\t$\\quad$ <b>DO the following <b>UNTIL</b> $\\statet$ is final:</b> <br>\n","\t\t$\\quad\\quad$ $\\actiont \\leftarrow \\pi(\\statet)$<br>\n","\t\t$\\quad\\quad$ $\\newstatet, \\reward \\leftarrow \\text{env.step}(\\actiont)$<br>\n","\t\t$\\quad\\quad$ $\\valuefunc(\\statet) \\leftarrow \\valuefunc(\\statet) + \\stepsize \\left[ \\reward + \\discount ~ \\valuefunc(\\newstatet) ~ - ~ \\valuefunc(\\statet) \\right]$<br>\n","\t\t$\\quad\\quad$ $\\statet \\leftarrow \\newstatet$<br>\n","\t$\\quad$ "]},{"cell_type":"markdown","metadata":{"id":"HrKoQype2z5x"},"source":["In the next cell, the TD-learning algorithm is prepared:\n","\n","**Notice**: New syntax of Gym (particularly, FrozenLake):\n","* ```s = env.reset()```: assign the starting state to variable ```s```\n","* ``` new_state, reward, is_final_state, info = env.step(A)```: sample the next state when doing action ```A```."]},{"cell_type":"code","metadata":{"id":"6tDSJ4o62z5y"},"source":["DISPLAY_EVERY_N_EPISODES = 50\n","def td_learning(policy, env, alpha_init=0.1, gamma=0.95, num_episodes=1000, display=False,alpha_factor=0.95):\n","    num_states = env.observation_space.n\n","    v_array = np.zeros(num_states)   # Initial value function\n","    alpha = alpha_init\n","    \n","    for episode_index in range(num_episodes):\n","        # For displaying purpose\n","        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n","            states_display(v_array, title=\"Value function (ep. {})\".format(episode_index), cbar=True, cmap=\"Reds\")\n","        else: print('.', end=\"\")\n","        value_function_history.append(v_array.copy())\n","        alpha_history.append(alpha)\n","        # End of displaying purpose            \n","\n","        \n","        state = env.reset()\n","        is_final_state = False\n","\n","        while not is_final_state:\n","            action = policy[state]\n","            new_state, reward, is_final_state, info = env.step(action)\n","            ##################\n","            # TD-LEARNING UPDATE\n","            v_array[state] = v_array[state] + alpha * (reward + gamma * v_array[new_state] - v_array[state])\n","            \n","            state = new_state\n","    \n","    return v_array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GT37eXf2z53"},"source":["## Exercise 1: **The role of step-size $\\alpha$:**\n"]},{"cell_type":"markdown","metadata":{"id":"Kos9ASJj2z5y"},"source":["\n","Now, we aim to test the ```td_learning``` algorithm by evaluating a specific policy (given in the next cell). \n","\n","*Remark:* This policy is actually the greedy_policy obtained from our (near) optimal value function in Lab1 (note again that in Lab1, the rewards and transition probablities are given a priori). The *target* values corresponding to this policy is: \n","<img src=\"https://raw.githubusercontent.com/dongquan-vu/teaching/main/Opt_Value_FrozenLake.PNG\" style=\"float: left\" width=\"400px\"/>\n"," \n","\n","\n","**In the next cell,** we run the td_learning algorithm and display the value function that it learnt. Then, we plot out the evolution of the value at each state and the evolution of the stepsize $\\alpha$.\n"]},{"cell_type":"code","metadata":{"id":"zbZiVWf82z50"},"source":["value_function_history = []\n","alpha_history = []\n","policy = [0, 3, 3, 3,\n","          0, 0, 0, 0,\n","          3, 1, 0, 0,\n","          0, 2, 1, 0]\n","env._max_episode_steps = 10000\n","# TD-Learning:\n","v_array = td_learning(policy, env, display=False)\n","env.close()\n","# Display the value function\n","states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")\n","# Plot out the evolution of value at each state\n","df_v_hist = pd.DataFrame(value_function_history)\n","df_v_hist.plot(figsize=(8,4))\n","plt.title(\"V(s) w.r.t iteration\")\n","plt.ylabel(\"V(s)\")\n","plt.xlabel(\"iteration\")\n","plt.legend(bbox_to_anchor=(1,1.2), ncol = 3);\n","plt.show()\n","\n","# Plotting the evolution of step-size alpha\n","plt.loglog(alpha_history)\n","plt.title(\"Alpha w.r.t iteration\")\n","plt.ylabel(\"Alpha\")\n","plt.xlabel(\"iteration\");\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JuuqRo1Lvix"},"source":["**QUESTION 1.1:** \n","\n","1/ What is your observation on the evolution of value approximation? \n","Do you think this is a good estimation of the target values given above? \n","\n","2/ Go back to the previous cell, run the TD-learning algorithm with smaller/larger parameter ```alpha```. How do you think ```alpha``` affects the evolution of values at states?\n","\n","3/ Go back to the cell defining ```td_learning``` algorithm and modify it such that the step-size alpha is a **decreasing sequence** as ```episode_index``` increases. Particularly, you can try several options as follows:\n","  * (a) ```alpha``` decreases linearly: at each ```episode_index```, set ```alpha =``` $\\frac{a}{1 + b \\cdot \\textrm{episode_index}}$ where $a < 1$ and $b$ are your chosen parameters. \n","  * (b) ```alpha``` decreases exponentially:  at each ```episode_index```, set ```alpha =``` $a^{\\textrm{episode_index}} $ where $a < 1$ is your chosen parameter (hint: you should choose $a$ closed to 1). \n","\n","Draw your conclusions on: what happens if the step-size $\\alpha$ decreases too fast? when it decreases too slow?"]},{"cell_type":"markdown","metadata":{"id":"R_hWvHlA5_3h"},"source":["**ANSWERS TO QUESTION 1.1:**\n","\n","\n","Please type your answer here. "]},{"cell_type":"markdown","metadata":{"id":"p3W92R5P2z57"},"source":["# Section 2: Tabular RL & Policy improvement\n"]},{"cell_type":"markdown","metadata":{"id":"IrCnxHLgHNgn"},"source":["## Preliminary functions:"]},{"cell_type":"markdown","metadata":{"id":"zSr0RXxlHBZg"},"source":["\n","In the next cell, two functions are coded for finding the nearly optimal policy based on the value estimated by td_learning. Particularly, we implement the *greedy* and the $\\epsilon$-*greedy* policies that agents will used to explore the environment and update their QTable:\n","\n","$Q^{\\pi}(s, a) =$ expected value when playing action $a$ at state $s$. \n","\n","QTable = a matrix (size $(|\\mathcal{S}| \\times |\\mathcal{A}|)$) storing the current estimated Q-value. \n","\n","\n","$\\displaystyle \\policy^{Q^{\\pi}}(\\statet) := \\text{greedy}(\\statet, Q^{\\pi}) = \\arg\\max_{\\actiont \\in \\actionspace} Q^{\\pi}(\\statet, \\actiont)$\n","\n","\n","$\\policy^{Q^{\\pi},\\epsilon}(\\statet) := \\epsilon\\text{-greedy}(\\statet, Q^{\\pi}) = $\n","randomly choose between $\\underbrace{\\text{greedy}(\\statet, Q^{\\pi})}_{\\text{with probability } 1 - \\epsilon}$\n","and $~~ \\underbrace{\\text{a random action}}_{\\text{with probability } \\epsilon}$"]},{"cell_type":"code","metadata":{"id":"Sp46Gu542z58"},"source":["def greedy_policy(state, q_array):\n","    action = np.argmax(q_array[state, :])\n","    return action\n","def epsilon_greedy_policy(state, q_array, epsilon):\n","    if np.random.rand() < epsilon:\n","        action = np.random.randint(q_array.shape[1])\n","    else: action = greedy_policy(state, q_array)\n","    return action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8Eh5Ppq2z59"},"source":["## Exercise 2: Implement the SARSA algorithm"]},{"cell_type":"markdown","metadata":{"id":"QuyyW_872z59"},"source":["To find the optimal policy (or a nearly optimal policy) for the FrozenLake-v0 problem, we will first use the SARSA algorithm. SARSA has two main elements\n","\n","* 1) Policy improvement: choose an action via $\\varepsilon$-greedy.\n","\n","* 2) Policy evaluation: Use TD-update (with respect to action chosen  $\\varepsilon$-greedy) for Q-value:\n","$$\n","Q^{\\pi}_{t+1} (s_t , a_t) \\leftarrow Q^{\\pi}_t(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q^{\\pi}_t(s_{t+1}, a_{t+1}) - Q^{\\pi}_t(s_t, a_t) \\right) ,\n","$$\n","$\\Rightarrow$ To estimate Q-value of policy $\\pi$ $\\Rightarrow$ this is called **On-policy** method.\n","\n","Particularly, a pseudo code of SARSA is as follow:\n","\n","---\n","SARSA\n","\n","\n","<b>Algorithm parameter</b>:\t$\\quad\\quad$ discount factor $\\discount$, \t$\\quad\\quad$ step size $\\stepsize \\in (0,1]$, \t$\\quad\\quad$ small $\\epsilon > 0$\n","\n","**Initialize** $\\qtable(\\state, \\action)=0, \\forall \\state \\in \\statespace$\n","\n","<b>FOR EACH</b> episode<br>\n","\t$\\quad$ $\\statet \\leftarrow \\text{env.reset}()$<br>\n","\t$\\quad$ $\\actiont \\leftarrow \\epsilon\\text{-greedy}(\\statet, Q)$<br>\n","$\\quad$ <b>DO the following <b>UNTIL</b> $\\statet$ is final:</b> <br>\t\t$\\quad\\quad$ $\\reward, \\newstatet \\leftarrow \\text{env.step}(\\actiont)$<br>\n","\t\t$\\quad\\quad$ $\\newactiont \\leftarrow \\epsilon\\text{-greedy}(\\newstatet, Q)$<br>\n","\t\t$\\quad\\quad$ $Q(\\statet,\\actiont) \\leftarrow Q(\\statet,\\actiont) + \\stepsize \\left[ \\underbrace{\\reward + \\discount ~ Q(\\newstatet,\\newactiont) ~ - ~ Q(\\statet,\\actiont)}_{\\text{TD error}} \\right]$<br>\n","\t\t$\\quad\\quad$ $\\statet \\leftarrow \\newstatet$<br>\n","\t\t$\\quad\\quad$ $\\actiont \\leftarrow \\newactiont$<br>"]},{"cell_type":"markdown","metadata":{"id":"dxSfxSQIKWmm"},"source":["**QUESTION 2.1**: In the next cell, you need to fill in the blank to implement SARSA. "]},{"cell_type":"code","metadata":{"id":"qO74bm9F2z5-"},"source":["DISPLAY_EVERY_N_EPISODES = 50\n","def sarsa(env, alpha_init=0.1, gamma=0.99, epsilon=0.5, num_episodes=10000, display=False):\n","    num_states = env.observation_space.n\n","    num_actions = env.action_space.n\n","    # Initial Q table (a matrix size num_states x num_actions)\n","    q_array = np.zeros([num_states, num_actions])   \n","    accum_reward = 0\n","    alpha = alpha_init\n","\n","    for episode_index in range(num_episodes):\n","        # FOR displaying results\n","        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n","            qtable_display(q_array, title=\"Q table\", cbar=True)\n","        else: print('.', end=\"\")\n","        q_array_history.append(q_array.copy())\n","        alpha_history.append(alpha)\n","        reward_history_sarsa.append(accum_reward)\n","        ##################\n","        ### BEGIN EXERCISE ###\n","\n","        ## Step 0: Write your update-rule of alpha here:\n","        alpha = #.......\n","\n","\n","        is_final_state = False\n","\n","        state = env.reset()\n","        action = epsilon_greedy_policy(state, q_array, epsilon)\n","\n","        while not is_final_state:\n","            # FILLING IN THE BLANK:\n","            #Step 1: sample a new_state and reward (and check if it is the final state) by env.step(action) \n","            # (hint: similar syntax to this step of td_learning )\n","\n","            # Step 2: Sample a new_action by  epsilon_greedy_policy with respect to the (q_array)\n","            new_action = #.........\n","            # Step 3: Update the q_array (at the state and action) via TD-update:\n","            q_array[state, action] = #..............\n","            ### END EXERCISE ###\n","         \n","            #Reassign state and action to new_state and new_action:\n","            state = new_state\n","            action = new_action\n","\n","        accum_reward+= reward  # For comparison purpose in Exercise 3\n","    return q_array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4DEI8JG2z5_"},"source":["**Task**: In the next cell, we run SARSA and assign its output to ```q_array```."]},{"cell_type":"code","metadata":{"id":"XfQXCG5Y2z5_"},"source":["q_array_history = []\n","alpha_history = []\n","reward_history_sarsa=[]\n","env._max_episode_steps = 1000\n","# Use SARSA to update Q_array:\n","q_array = sarsa(env, epsilon=0.5, display=False)\n","env.close()\n","#Display the Q_table:\n","qtable_display(q_array, title=\"Q Table\", cbar=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XaAhcQlP2z6E"},"source":["### Evaluate Policy with Gym"]},{"cell_type":"markdown","metadata":{"id":"mm9SHpEw2z6F"},"source":["Run the next cell to count the number of successful trials of SARSA on 1000 episodes.\n","\n","**Note**: OpenAI considers the task is solved if you reach 76\\% of success over the episodes."]},{"cell_type":"code","metadata":{"id":"W4_hYFki2z6F"},"source":["env = gym.make('FrozenLake-v0')\n","env._max_episode_steps = 2000\n","\n","reward_list = []\n","NUM_EPISODES = 5000\n","\n","for episode_index in range(NUM_EPISODES):\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        action = greedy_policy(state, q_array)\n","        state, reward, done, info = env.step(action)\n","    reward_list.append(reward)\n","reward_df = pd.DataFrame(reward_list)\n","print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Qcw2U7C2z6G"},"source":["### Exercise 3: Implement the QLearning algorithm"]},{"cell_type":"markdown","metadata":{"id":"5m_10Pgz2z6H"},"source":["## Exercise 3: Implement the Q-LEARNING algorithm\n"]},{"cell_type":"markdown","metadata":{"id":"sv-m46mtHGI4"},"source":["SARSA is an on-policy method. In this exercise, we implement Q-learning: an **off-policy** method: Q-learning doesn't estimate the Q-function of its current policy but it estimates the value *the optimal one*.\n","\n","To do so, it uses the following update rule:\n","$$\n","Q_{t+1}(S_t,A_t) \\leftarrow Q_t(S_t, A_t) + \\alpha \\left( r_t + \\gamma \\underbrace{\\max_a Q_t(S_{t+1}, a)}_{\\textrm{different from SARSA}} - Q_t(S_t, A_t) \\right) .\n","$$\n","\n","Intuitively, Q-learning is \"being greedy\" and chooses the $\\textrm{argmax}_a Q(S,a)$ action (instead of choosing the next action by current policy like SARSA).\n","\n","---\n","QLearning\n","\n","\n","<b>Algorithm parameter</b>:\n","\t$\\quad\\quad$ discount factor $\\discount$,\n","\t$\\quad\\quad$ step size $\\stepsize \\in (0,1]$ ,\n","\t$\\quad\\quad$ small $\\epsilon > 0$,\n","\t\n","**Initialize** $\\qtable(\\state, \\action) =0, ~~~ \\forall \\state \\in \\statespace, \\action \\in \\actionspace(\\state)$<br>\n","\n","<b>FOR EACH</b> episode<br>\n","\t$\\quad$ $\\statet \\leftarrow \\text{env.reset}()$<br>\n","\t$\\quad$ <b>DO the following  <b>UNTIL</b> $\\statet$ is final:</b> <br>\n","\t\t$\\quad\\quad$ $\\actiont \\leftarrow \\epsilon\\text{-greedy}(\\statet, Q)$<br>\n","\t\t$\\quad\\quad$ $\\reward, \\newstatet \\leftarrow \\text{env.step}(\\actiont)$<br>\n","\t\t$\\quad\\quad$ $Q(\\statet,\\actiont) \\leftarrow Q(\\statet,\\actiont) + \\stepsize \\left[ \\underbrace{\\reward + \\discount ~ \\max_{\\action} Q(\\newstatet, \\action) ~ - ~ Q(\\statet,\\actiont)}_{\\text{TD error}} \\right]$<br>\n","\t\t$\\quad\\quad$ $\\statet \\leftarrow \\newstatet$<br>\n","\t"]},{"cell_type":"markdown","metadata":{"id":"tMuFzoN6EKFp"},"source":["**QUESTION 3.1.** In the next cell, you need to fill in the blank to implement the Q-LEARNING algorithm:"]},{"cell_type":"code","metadata":{"id":"KXaWUa6t2z6H"},"source":["DISPLAY_EVERY_N_EPISODES = 50\n","\n","def q_learning(env, alpha_init=0.1, gamma=0.99, epsilon=0.5, num_episodes=10000, display=False):\n","    num_states = env.observation_space.n\n","    num_actions = env.action_space.n\n","    q_array = np.zeros([num_states, num_actions])   # Initial Q table\n","    accum_reward = 0\n","    alpha = alpha_init\n","\n","    for episode_index in range(num_episodes):\n","        # For displaying purpose:  \n","        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n","            qtable_display(q_array, title=\"Q table\", cbar=True)\n","        else: print('.', end=\"\")\n","        q_array_history.append(q_array.copy())\n","        alpha_history.append(alpha)\n","        reward_history_qlearning.append(accum_reward)\n","        \n","        \n","        ### BEGIN EXERCISE ###\n","\n","        # Step 0:Write your update-rule of stepsize alpha here\n","        alpha = #...........\n","\n","        is_final_state = False\n","        state = env.reset()\n","\n","        while not is_final_state:\n","            # We take action from epsilon-greedy INSIDE the while-loop (unlike SARSA doing this outside of while-loop)\n","            action = epsilon_greedy_policy(state, q_array, epsilon)\n","            # Filling in the blank:\n","            # Step 1: Sample new_state, reward via env.step(action):\n"," \n"," \n","            # Step 2: Find the max_action = max_a (q_array[new_state, a]). \n","            # Hints: you can use the greedy_policy function defined above:         \n","            max_action = #............\n","            # Step 3: TD-Update with max_action:\n","            q_array[state, action] = #..............\n","\n","            ### END EXERCISE ###\n","\n","\n","\n","            # Re-assign new_state to state:\n","            state = new_state\n","        \n","        accum_reward += reward  # For comparison purpose in Bonus question\n","    return q_array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nd1KWgfXXBA9"},"source":["**Task**: In the next cell, we run Q_LEARNING and assign its output to ```q_array```."]},{"cell_type":"code","metadata":{"id":"wLRPfn1O2z6J"},"source":["q_array_history = []\n","alpha_history = []\n","reward_history_qlearning=[]\n","\n","env._max_episode_steps = 1000\n","q_array = q_learning(env, display=False)\n","env.close()\n","qtable_display(q_array, title=\"Q Table\", cbar=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpkFaQLJ2z6O"},"source":["### Evaluate Policy with Gym"]},{"cell_type":"markdown","metadata":{"id":"l3-2GMvm2z6P"},"source":["As a measure of performance, run the next cell to count the number of successful trials of Q-learning on 1000 episodes.\n","\n","**Note**: OpenAI considers the task is solved if you reach 76\\% of success over the episodes."]},{"cell_type":"code","metadata":{"id":"aCNrpgS22z6P"},"source":["environment = gym.make('FrozenLake-v0')\n","environment._max_episode_steps = 1000\n","reward_list = []\n","NUM_EPISODES = 5000\n","for episode_index in range(NUM_EPISODES):\n","    state = environment.reset()\n","    done = False\n","    #t = 0\n","    while not done:\n","        #action = epsilon_greedy_policy(state, q_array, epsilon)\n","        action = greedy_policy(state, q_array)\n","        state, reward, done, info = environment.step(action)\n","    reward_list.append(reward)\n","reward_df = pd.DataFrame(reward_list)\n","print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n","environment.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kifYBFTmDrsX"},"source":["**Question**: Which algorithm you think performs better SARSA or Q-LEARNING (given the same number of training episodes and alpha update rule)? (To mitigate the randomness, we should check the success rate in a large number of episodes)."]},{"cell_type":"markdown","metadata":{"id":"Qj1EeNbXbzhV"},"source":["### Bonus question: Compare SARSA and Q-LEARNING in training:\n","In the next cell, we plot out the accumulated reward while **training** the algorithms. Which one do you think perform worse in the beginning of the training? Any explanation for this?\n","\n","CONCLUSION:\n","* Use SARSA when we \"care\" about the performance while training\n","* Use Q-Learning when we \"do not care\" about the performance while training.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kB4SN-0YaIXW"},"source":["f,ax = plt.subplots()\n","ax.plot(reward_history_qlearning)\n","ax.plot(reward_history_sarsa)\n","ax.legend(['q_learning', 'sarsa'],bbox_to_anchor=(1, 0.5))\n","ax.set_ylabel('accumulated reward')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v4aCBmD9DiGZ"},"source":["**ANSWERS FOR BONUS QUESTION**:\n","\n","Please write your answer here."]}]}