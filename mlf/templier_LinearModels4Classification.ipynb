{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron, Adaline and Logistique Regression\n",
    "\n",
    "## by *Templier William*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the implementation of the perceptron algorithm (Rosenblatt, 68), Adaline (Widrow et Hoff, 60) and Logisitc Regression (Cox, 66) whose pseudo-code are the following:\n",
    "\n",
    "Perceptron:\n",
    "`Input: Train, eta, MaxEp\n",
    "init: w\n",
    "epoch = 0\n",
    "err = 1\n",
    "m = len(Train)\n",
    "while epoque <= MaxEp and err! = 0\n",
    "    err = 0\n",
    "    for i in 1: m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w * x\n",
    "        if (y * h <= 0)\n",
    "            w <- w + eta * y * x\n",
    "            err <- err + 1\n",
    "     epoch <- epoch + 1\n",
    "output: w`\n",
    "\n",
    "Adaline:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*(y-dp)*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "Logistic Regression:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "1. Create a list of 4 elements corresponding to the logical AND example called `Train`:\n",
    "$Train=\\{((1,+1,+1),+1),((1,-1,+1),-1),((1,-1,-1),-1),((1,+1,-1),-1)\\}$\n",
    "\n",
    "Each element of the list is a list which last characteristic is the class of the example and the first characteristics their coordinates with the biais '1' included at the beginning of each vector.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 1, 1], 1), ([1, -1, 1], -1), ([1, -1, -1], -1), ([1, 1, -1], -1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train=[\n",
    "    ([1, +1, +1], +1),\n",
    "    ([1, -1, +1], -1),\n",
    "    ([1, -1, -1], -1),\n",
    "    ([1, +1, -1], -1),\n",
    "]\n",
    "Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Code the Perceptron, Adaline and LR (Logistic regression) programs\n",
    "\n",
    "Hint: You can write a function that calculates the dot product between an example $\\mathbf{x} = (x_1, \\ldots, x_d)$ and the weight vector $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)$: \n",
    "$ h(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_ {j = 1} ^ d w_j x_j $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    return sum(x_i*y_i for x_i, y_i in zip(x, y))\n",
    "\n",
    "from math import exp\n",
    "\n",
    "def sigmoid(z):\n",
    "    return (1.0/(1.0+exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the only difference between the three algorithms was a *term* in the update segmnent (w <- w + x \\* term \\* eta), I refactored as much as possible, to be able to write a simple **gradient descent** algorithm that can be used by prodiving a desired `update_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_update(w, x, term, eta):\n",
    "    return [w_i + eta * term * x_i for w_i,x_i in zip(w,x)]\n",
    "\n",
    "def perceptron_update(w,x,h,y,eta):\n",
    "    #w <- w + eta * y * x\n",
    "    return _compute_update(w, x, term=y, eta=eta)\n",
    "\n",
    "def adaline_update(w,x,h,y,eta):\n",
    "    #w <- w + eta*(y-dp)*x\n",
    "    return _compute_update(w, x, term=(y - h), eta=eta)\n",
    "\n",
    "def logreg_update(w,x,h,y,eta):\n",
    "    #w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "    return _compute_update(w,x, term=(y * (1 - sigmoid(y * h))), eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def gradient_descent(train, eta, max_epoch, update_rule, update_on_error=False):\n",
    "    w = [0.0] * len(train)\n",
    "    #err = 1\n",
    "    epoch = 0\n",
    "    \n",
    "    while (epoch < max_epoch): #and err > 0:\n",
    "        err = 0\n",
    "        \n",
    "        for i in range(len(train)):\n",
    "            x,y = train[randint(0, len(train)-1)]\n",
    "            #print(max(x)) for checking the overflow error\n",
    "            h = dot(w,x)\n",
    "\n",
    "            if (y * h <= 0): err += 1\n",
    "\n",
    "            if err > 0 and update_on_error: # for the perceptron\n",
    "                w = update_rule(w,x,h,y,eta)\n",
    "\n",
    "            elif not update_on_error:\n",
    "                w = update_rule(w,x,h,y,eta)\n",
    "                \n",
    "        epoch += 1\n",
    "                \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_perceptron = gradient_descent(Train, 0.01, 500, perceptron_update, update_on_error=True)\n",
    "w_adaline = gradient_descent(Train, 0.01, 500, adaline_update)\n",
    "w_logreg = gradient_descent(Train, 0.01, 500, logreg_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply the three learning models on the logical AND, and calculate the model error rate on this basis.\n",
    "\n",
    "Hint: You can write a function that takes a weight vector $\\mathbf{w}$ and an example $(\\mathbf{x},y)$ and calculates the error rate of the model with weight $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = [\n",
    "    ([1, +1, +1], +1),\n",
    "    ([1, -1, +1], -1),\n",
    "    ([1, -1, -1], -1),\n",
    "    ([1, +1, -1], -1),\n",
    "    ([1, +1, +1], +1),\n",
    "    ([1, -1, +1], -1),\n",
    "    ([1, -1, -1], -1),\n",
    "    ([1, +1, -1], -1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmpiricalRisk(Test,W):\n",
    "    E=0.0\n",
    "    m=len(Test)\n",
    "    # The empirical error of a model with weight W on a test set of size m\n",
    "    for xi, yi in Test:\n",
    "        h_w = dot(W, xi)\n",
    "        if (yi * h_w <= 0):\n",
    "            E+=1.0\n",
    "    return E/float(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical risk Perceptron = 0.0\n",
      "Empirical risk Adaline = 0.0\n",
      "Empirical risk LogReg = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Empirical risk Perceptron = {EmpiricalRisk(Test, w_perceptron)}\")\n",
    "print(f\"Empirical risk Adaline = {EmpiricalRisk(Test, w_adaline)}\")\n",
    "print(f\"Empirical risk LogReg = {EmpiricalRisk(Test, w_logreg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We are now going to focus on the behavior of the three models on http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks), https://archive.ics.uci.edu/ml/datasets/spambase, https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29, https://archive.ics.uci.edu/ml/datasets/Ionosphere. These files are in the current respository with the names `sonar.txt`; `spam.txt`; `wdbc.txt` and `ionoshpere.txt`. We can use the following `ReadCollection` function in order to read the files in the form of the training set that is requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Normalize(x):\n",
    "    norm=0.0\n",
    "    for e in x:\n",
    "        norm+=e**2\n",
    "    for i in range(len(x)):\n",
    "        x[i]/=sqrt(norm)\n",
    "    return x\n",
    "\n",
    "def ReadCollection(filename, normalize=True):\n",
    "    tag_df=pd.read_table(f'data/{filename}',sep=',',header=None)\n",
    "    if(\"wdbc\" in filename):\n",
    "        Dic={'M': -1, 'B': +1}\n",
    "    elif(\"sonar\" in filename):\n",
    "        Dic={'R': -1, 'M': +1}\n",
    "    elif(\"iono\" in filename):\n",
    "        Dic={'g': -1, 'b': +1}\n",
    "    elif(\"spam\" in filename):\n",
    "        Dic={0:-1, 1:+1}\n",
    "        \n",
    "    X=[]\n",
    "    for e in range(len(tag_df)):\n",
    "        x=list(tag_df.loc[e,:])\n",
    "        if(\"wdbc\" in filename):\n",
    "            x.pop(0)\n",
    "            cls=x.pop(0)\n",
    "        else:\n",
    "            cls=x.pop()\n",
    "            \n",
    "        if normalize: x=Normalize(x)\n",
    "        x.insert(len(x),Dic[cls])\n",
    "        X.append(x)\n",
    "    random.shuffle(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Run the three models on these files with $\\eta=0.01$ et $\\eta=0.1$ and `MaxEp=500`.\n",
    " \n",
    " 3. Report in the table below the average of the error rates on the test by repeating each experiment 10 times. \n",
    " \n",
    " <br>\n",
    " \n",
    " \n",
    " <center> $\\eta=0.01$, MaxE$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  | WDBC       |            |         |          |\n",
    "  | Ionosphere |            |         |          |\n",
    "  | Sonar      |            |         |          |\n",
    "  | Spam       |            |         |          |\n",
    " \n",
    " <br><br>\n",
    "  \n",
    "  <center> $\\eta=0.1$, MaxEp$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  | WDBC       |            |         |          |\n",
    "  | Ionosphere |            |         |          |\n",
    "  | Sonar      |            |         |          |\n",
    "  | Spam       |            |         |          |\n",
    "  \n",
    "  Hint: you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Perceptron': perceptron_update,\n",
    "    'Adaline': adaline_update,\n",
    "    'LogReg': logreg_update\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to transform the dataset in (xi,yi) tuples\n",
    "def get_x_y(dataset):\n",
    "    return [(vector[:-1], vector[-1]) for vector in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters_tuning(etas=[0.01,0.1], normalize=True, nb_iter=10):\n",
    "\n",
    "    datasets = {\n",
    "        \"WDBC\": ReadCollection(\"wdbc.txt\", normalize=normalize), \n",
    "        \"Ionosphere\": ReadCollection(\"ionosphere.txt\", normalize=normalize),\n",
    "        \"Sonar\": ReadCollection(\"sonar.txt\", normalize=normalize),\n",
    "        \"Spam\": ReadCollection(\"spam.txt\", normalize=normalize)\n",
    "    }\n",
    "\n",
    "    for eta in etas:\n",
    "        print(f'eta = {eta}\\n--------')\n",
    "\n",
    "        results = pd.DataFrame(\n",
    "                index=datasets.keys(),\n",
    "                columns=models.keys(),\n",
    "                dtype=float\n",
    "        ).fillna(0.0)\n",
    "\n",
    "        for name, X in datasets.items():\n",
    "            for i in range(nb_iter):\n",
    "                train, test = train_test_split(X, test_size=0.25)\n",
    "\n",
    "                train = get_x_y(train)\n",
    "                test = get_x_y(test)\n",
    "\n",
    "                for model, update_fnc in models.items():\n",
    "                    results.loc[name, model] += EmpiricalRisk(\n",
    "                        test, gradient_descent(train, eta, 500, update_fnc)\n",
    "                    )\n",
    "\n",
    "        print(results.applymap(lambda x: round(x/float(nb_iter), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 0.01\n",
      "--------\n",
      "            Perceptron  Adaline  LogReg\n",
      "WDBC            0.3787   0.0843  0.0965\n",
      "Ionosphere      0.3068   0.1648  0.1687\n",
      "Sonar           0.4452   0.2856  0.2846\n",
      "Spam            0.3929   0.2602  0.2286\n",
      "eta = 0.1\n",
      "--------\n",
      "            Perceptron  Adaline  LogReg\n",
      "WDBC            0.3762   0.0913  0.0916\n",
      "Ionosphere      0.2994   0.1750  0.1619\n",
      "Sonar           0.4846   0.2452  0.2279\n",
      "Spam            0.3935   0.2581  0.1514\n"
     ]
    }
   ],
   "source": [
    "hyperparameters_tuning(etas=[0.01,0.1], normalize=True, nb_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Normalize the vector representations of observataions by dividing them with their norm and repeat quetions 2 and 3. Are there any significant change by normalizing? Please explain.\n",
    " \n",
    " **We inverted here, as the data were already normalized. In the following we discuss the not normalized data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 0.01\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48/1858308193.py:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return sum(x_i*y_i for x_i, y_i in zip(x, y))\n",
      "/tmp/ipykernel_48/3364597621.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return [w_i + eta * term * x_i for w_i,x_i in zip(w,x)]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/2138284269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhyperparameters_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48/3878634033.py\u001b[0m in \u001b[0;36mhyperparameters_tuning\u001b[0;34m(etas, normalize, nb_iter)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     results.loc[name, model] += EmpiricalRisk(\n\u001b[0;32m---> 28\u001b[0;31m                         \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/790527145.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(train, eta, max_epoch, update_rule, update_on_error)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/3364597621.py\u001b[0m in \u001b[0;36mlogreg_update\u001b[0;34m(w, x, h, y, eta)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogreg_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#w <- w + eta*y*(1-sigm(y*dp))*x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compute_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48/1858308193.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "hyperparameters_tuning(etas=[0.01,0.1], normalize=False, nb_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we upgrade the weights through gradient descent, using a vector **x** that can have high values - we cumulate higher and higher values - especially when doing the dot product *h*.\n",
    "\n",
    "In the update function of the Logistic regression, we use the `exp` function on the math library which does not handle overflow when the input value is really large (and thus gets exponentially larger). We could use a **exp** function from a library that handles overflows (by recasting for example), like *numpy*.\n",
    "\n",
    "It is also know that normalized data facilitates the gradient descent convergence by *smoothing* the space, as shown in the image below - taken from [here](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/700/1*vXpodxSx-nslMSpOELhovg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"https://miro.medium.com/max/700/1*vXpodxSx-nslMSpOELhovg.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
